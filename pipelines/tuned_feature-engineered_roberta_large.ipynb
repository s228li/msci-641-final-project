{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq transformers datasets evaluate","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:58:40.967618Z","iopub.execute_input":"2023-08-04T02:58:40.968398Z","iopub.status.idle":"2023-08-04T02:58:54.231213Z","shell.execute_reply.started":"2023-08-04T02:58:40.968357Z","shell.execute_reply":"2023-08-04T02:58:54.229824Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")\nimport numpy as np \nimport pandas as pd \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import Dataset\nimport torch\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-04T02:58:54.233654Z","iopub.execute_input":"2023-08-04T02:58:54.234025Z","iopub.status.idle":"2023-08-04T02:59:09.529647Z","shell.execute_reply.started":"2023-08-04T02:58:54.233987Z","shell.execute_reply":"2023-08-04T02:59:09.528682Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/clickbait-detection-msci641-s23'","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:09.530923Z","iopub.execute_input":"2023-08-04T02:59:09.532505Z","iopub.status.idle":"2023-08-04T02:59:09.537443Z","shell.execute_reply.started":"2023-08-04T02:59:09.532465Z","shell.execute_reply":"2023-08-04T02:59:09.536484Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_json(f'{data_path}/train.jsonl', lines=True)\ntest = pd.read_json(f'{data_path}/test.jsonl', lines=True)\nval = pd.read_json(f'{data_path}/val.jsonl', lines=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:09.539946Z","iopub.execute_input":"2023-08-04T02:59:09.540670Z","iopub.status.idle":"2023-08-04T02:59:10.203087Z","shell.execute_reply.started":"2023-08-04T02:59:09.540636Z","shell.execute_reply":"2023-08-04T02:59:10.202100Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:10.206375Z","iopub.execute_input":"2023-08-04T02:59:10.208964Z","iopub.status.idle":"2023-08-04T02:59:11.334592Z","shell.execute_reply.started":"2023-08-04T02:59:10.208934Z","shell.execute_reply":"2023-08-04T02:59:11.333577Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a45043bdc0a452daa1f867c18adb53b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c358eefb174f3cb5e1762754723d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05038e0b44c94148abefc479c277efff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5609d9b9944479781a2c94dabec07d7"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:11.336088Z","iopub.execute_input":"2023-08-04T02:59:11.336607Z","iopub.status.idle":"2023-08-04T02:59:11.342748Z","shell.execute_reply.started":"2023-08-04T02:59:11.336566Z","shell.execute_reply":"2023-08-04T02:59:11.341760Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"id2label = {0: 'passage', 1: 'phrase', 2: 'multi'}\nlabel2id = {'passage': 0, 'phrase': 1, 'multi': 2}","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:11.344364Z","iopub.execute_input":"2023-08-04T02:59:11.345413Z","iopub.status.idle":"2023-08-04T02:59:11.355977Z","shell.execute_reply.started":"2023-08-04T02:59:11.345378Z","shell.execute_reply":"2023-08-04T02:59:11.355018Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(df, test=False):\n    ret = []\n    if test:\n        for _, i in df.iterrows():\n            platform = i['postPlatform']\n            post = ' '.join(i['postText'])\n            title = i['targetTitle']\n            description = i['targetDescription']\n            paragraph = ' '.join(i['targetParagraphs'])\n            keyword = i['targetKeywords']\n            text = f\"{platform} Post: {post} Website Title: {title} Website Description: {description} Website Paragraph: {paragraph} Website Keyword: {keyword}\"\n            ret += [{'text': text, 'id': i['id']}]\n            ret_df = pd.DataFrame(ret)\n        \n    else:\n        for _, i in df.iterrows():\n            platform = i['postPlatform']\n            post = ' '.join(i['postText'])\n            title = i['targetTitle']\n            description = i['targetDescription']\n            paragraph = ' '.join(i['targetParagraphs'])\n            keyword = i['targetKeywords']\n            text = f\"{platform} Post: {post} Website Title: {title} Website Description: {description} Website Paragraph: {paragraph} Website Keyword: {keyword}\"\n            ret += [{'text': text, 'labels': i['tags'][0]}]\n            ret_df = pd.DataFrame(ret)\n            ret_df['labels'] = ret_df['labels'].apply(lambda x: label2id[x])\n    \n    data = Dataset.from_pandas(ret_df)\n    tokenized_data = data.map(tokenize, batched=True)\n    return tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:11.357479Z","iopub.execute_input":"2023-08-04T02:59:11.357864Z","iopub.status.idle":"2023-08-04T02:59:11.371421Z","shell.execute_reply.started":"2023-08-04T02:59:11.357831Z","shell.execute_reply":"2023-08-04T02:59:11.370255Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df = preprocess_data(train)\nval_df = preprocess_data(val)\ntest_df = preprocess_data(test, test=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:11.373007Z","iopub.execute_input":"2023-08-04T02:59:11.373348Z","iopub.status.idle":"2023-08-04T02:59:30.546389Z","shell.execute_reply.started":"2023-08-04T02:59:11.373317Z","shell.execute_reply":"2023-08-04T02:59:30.545452Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8599438a97b64b1e88c0bd794a85113c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ca2a56695648fa930b3ce27cb31892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c3f3ab3f144a7a87be9687ac9db808"}},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\nf1 = evaluate.load(\"f1\", average='macro')\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return f1.compute(predictions=predictions, references=labels, average='macro')","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:30.550664Z","iopub.execute_input":"2023-08-04T02:59:30.551275Z","iopub.status.idle":"2023-08-04T02:59:33.093929Z","shell.execute_reply.started":"2023-08-04T02:59:30.551238Z","shell.execute_reply":"2023-08-04T02:59:33.092992Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62bd697d5858468dbd13280e6d46cb71"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-large\", num_labels=3, id2label=id2label, label2id=label2id\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:33.095158Z","iopub.execute_input":"2023-08-04T02:59:33.095490Z","iopub.status.idle":"2023-08-04T02:59:43.485709Z","shell.execute_reply.started":"2023-08-04T02:59:33.095455Z","shell.execute_reply":"2023-08-04T02:59:43.484101Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e6171b73e324b7489165324cf367574"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:43.487182Z","iopub.execute_input":"2023-08-04T02:59:43.487554Z","iopub.status.idle":"2023-08-04T02:59:43.497274Z","shell.execute_reply.started":"2023-08-04T02:59:43.487500Z","shell.execute_reply":"2023-08-04T02:59:43.492010Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    #gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    fp16=True,\n    num_train_epochs=50,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:43.499166Z","iopub.execute_input":"2023-08-04T02:59:43.502797Z","iopub.status.idle":"2023-08-04T02:59:44.078369Z","shell.execute_reply.started":"2023-08-04T02:59:43.502756Z","shell.execute_reply":"2023-08-04T02:59:44.077134Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_df,\n    eval_dataset=val_df,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:44.083589Z","iopub.execute_input":"2023-08-04T02:59:44.086402Z","iopub.status.idle":"2023-08-04T02:59:51.104121Z","shell.execute_reply.started":"2023-08-04T02:59:44.086358Z","shell.execute_reply":"2023-08-04T02:59:51.103090Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:59:51.105569Z","iopub.execute_input":"2023-08-04T02:59:51.106231Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='41' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  41/5000 11:21 < 24:04:04, 0.06 it/s, Epoch 0.40/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.173200</td>\n      <td>1.142088</td>\n      <td>0.288052</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.120900</td>\n      <td>1.097181</td>\n      <td>0.192171</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.088800</td>\n      <td>1.061802</td>\n      <td>0.192171</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.045000</td>\n      <td>1.069235</td>\n      <td>0.192171</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"pred = trainer.predict(test_df).predictions\npred_ids = np.argmax(pred, 1)\ntest['spoilerType'] = pred_ids\ntest['spoilerType'] = test['spoilerType'].apply(lambda x: id2label[x])\nsubmissions = test[['id', 'spoilerType']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions.to_csv('submissions.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}